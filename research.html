<!DOCTYPE HTML>
<!--
Telephasic by HTML5 UP
html5up.net | @ajlkn
Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
    <title>Roshni Kaushik</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="assets/css/bootstrap.min.css">
    <link rel="stylesheet" href="assets/css/main.css" />
</head>

<body class="no-sidebar is-preload">
    <div id="page-wrapper">

        <!-- Header -->
        <div id="header-wrapper">
            <div id="header" class="container">

                <!-- Logo -->
                <h1 id="logo"><a href="index.html">Roshni Kaushik</a></h1>

                <!-- Nav -->
                <nav id="nav">
                    <ul>
                        <li>
                            <a href="experience.html">Experience</a>
                        </li>
                        <li>
                            <a href="research.html">Research</a>
                        </li>
                        <li class="break">
                            <a href="educationskills.html">Education/Skills</a>
                        </li>
                        <li>
                            <a href="blog.html">Blog</a>
                        </li>
                    </ul>
                </nav>

            </div>
        </div>

        <!-- Main -->
        <div class="wrapper">
            <div class="container" id="main">

                <!-- Content -->
                <article id="content">
                    <header>
                        <h2>Research</h2>
                    </header>
                    <div class="accordion" id="accordionExample">
                        <div class="card">
                            <div class="card-header" id="exercisecoach">
                                <h5 class="mb-0">
                                    <button class="btn btn-link" type="button" data-toggle="collapse" data-target="#collapseexercise" aria-expanded="true" aria-controls="collapseexercise">
                                        <div style="white-space:normal">
                                        <span class="float-left">Personalized Context-aware Affective Nonverbal Feedback: 2022 - Current </span>
                                        <span class="float-right"><i class="fa"></i></span>
                                        </div>
                                    </button>
                                </h5>
                            </div>

                            <div id="collapseexercise" class="collapse show" aria-labelledby="exercisecoach" data-parent="#accordionExample">
                                <div class="card-body">
                                    <p>Different people respond to feedback and guidance in different ways. Some might prefer a firmer approach, some might prefer softer. Their preferences may even change depending on their mood, physical health, etc. This
                                        project aims to learn people's preferences for both verbal and non-verbal feedback and guidance, by observing their reactions to different types of feedback as they perform various activities in various contexts
                                        (e.g., progress in the session, physical and emotional state, etc.). We will use mainly non-verbal cues from people - facial expressions, posture, gestures - to infer their mental state and the AI agent will respond,
                                        in kind, with facial expressions, posture, gestures, and verbal feedback and guidance. </p>
                                    <a href="#" style="width: 100%; max-width: 600; height: auto; display: block; margin: 0 auto; " class="image featured"><img src="images/diagram.PNG" alt="" /></a>

                                    <p> This work is being done under <a href="https://www.ri.cmu.edu/ri-faculty/reid-simmons/">Dr. Reid Simmons</a> at Carnegie Mellon University. All work is funded through the <a href="https://www.ai-caring.org/">AI-CARING project</a>.</p>

                                    <b>Related Publications </b>
                                    <p>[Workshop] <b>R. Kaushik</b> and R. Simmons, <a href="https://drive.google.com/file/d/1RiZsQO_WiDUjAmfohY4_TWg9FRtVMaWE/view">"Contextual Bandit Approach to Generating Robot Feedback for Human Exercise"</a>, HRI 2024
                                    </p>
                                </div>
                            </div>
                        </div>

                        <div class="card">
                            <div class="card-header" id="teachablerobots">
                                <h5 class="mb-0">
                                    <button class="btn btn-link collapsed" type="button" data-toggle="collapse" data-target="#collapseOne" aria-expanded="false" aria-controls="collapseOne">
                                        <div style="white-space:normal">
                                        <span class="float-left">Teachable Robots: 2019 - 2022 &nbsp &nbsp</span>
                                        <span class="float-right"><i class="fa"></i></span>
                                        </div>
                                    </button>
                                </h5>
                            </div>

                            <div id="collapseOne" class="collapse" aria-labelledby="teachablerobots" data-parent="#accordionExample">
                                <div class="card-body">
                                    <p>Peer tutoring, in which one student teaches material to another student, has been shown to increase learning. In particular, there can be significant learning gains for the person taking on the role of the teacher if
                                        they can engage in reflective knowledge building as they tutor. Unfortunately, it is difficult to find other students who can play this role and it is likely that students would tend not to believe it if adults
                                        played the role. We propose that a suitable social robot could believably play the role of ignorant learner, while in reality it would already understand the concept being taught and so could subtly guide the student
                                        teacher with its behaviors and modes of interaction (often referred to as “back leading”).</p>

                                    <p> This work is being done under <a href="https://www.ri.cmu.edu/ri-faculty/reid-simmons/">Dr. Reid Simmons</a> at Carnegie Mellon University. See our project website <a href="http://www.cs.cmu.edu/~teachable/">Multi-Modal Communications for Teachable Robots</a>.
                                        All work is funded by NSF.</p>

                                    <b>Related Publications </b>
                                    <p>[Conference] <b>R. Kaushik</b> and R. Simmons, "<a href="https://par.nsf.gov/servlets/purl/10401925">Affective Robot Behavior Improves Learning in a Sorting Game</a>", RO-MAN 2022
                                    </p>
                                    <p>[Workshop] <b>R. Kaushik</b> and R. Simmons, "<a href="https://drive.google.com/file/d/1lL1htgJquM0SV8m4Q3tRm_wJbzRY0Wdp/view?usp=sharing">Context-dependent Personalized Robot Feedback to Improve Learning</a>", Context-awareness in HRI Workshop (part of HRI 2022)
                                    </p>
                                    <p>[Conference] <b>R. Kaushik</b> and R. Simmons, "<a href="https://dl.acm.org/doi/abs/10.1145/3434074.3447129">Perception of Emotion in Torso and Arm Movements on Humanoid Robot Quori</a>", HRI 2021. </p>
                                    <p>[Conference] <b>R. Kaushik</b> and R. Simmons,"<a href="https://link.springer.com/chapter/10.1007/978-3-030-90525-5_26">Early Prediction of Student Engagement-related Events from Facial and Contextual Features"</a>, ICSR 2021. </p>
                                </div>
                            </div>
                        </div>
                        <div class="card">
                            <div class="card-header" id="masterswork">
                                <h5 class="mb-0">
                                    <button class="btn btn-link collapsed" type="button" data-toggle="collapse" data-target="#collapseTwo" aria-expanded="false" aria-controls="collapseTwo">
                                        <div style="white-space:normal">
                                        <span class="float-left">Imitating Human Movement Using Non-humanoid Virtual Characters: 2017 - 2019 &nbsp &nbsp</span>
                                        <span class="float-right"><i class="fa"></i></span>
                                        </div>
                                    </button>
                                </h5>
                            </div>
                            <div id="collapseTwo" class="collapse" aria-labelledby="masterswork" data-parent="#accordionExample">
                                <div class="card-body">
                                    <p>How should two bodies of different morphology move to imitate one another? While exact replication of activity is not possible, particularly when one system has many fewer degrees-of-freedom than the other, we can define
                                        perceptual imitation to be achieved when human viewers see the same activity between the two bodies. Even between humans, our differing mobility and limb lengths create differences in the execution of a task. Yet,
                                        even with these differences, we imitate each other in many situations including children learning newmovements from the adults around them, people in exercise classes following the movements of an instructor, or
                                        pedestrians taking cues from the people around them on the sidewalk. Moreover, simple cartoon characters and robots are frequently seen as “doing the same thing” as natural counterparts. These examples show that
                                        this perceptual imitation is possible and common.</p>
                                    <video style="width: 100%; max-width: 800px; height: auto; display: block; margin: 0 auto; " controls>
                                    <source src="images/broombot.mp4" type="video/mp4">
                                    </video>
                                    <br>
                                    <p>Take a look at this short animation and compare the movement of these two "Broom" robots. The character on the left is mirroring the "verticality" or leaning of the spine of the human motion capture skeleton in the
                                        center. The robot on the right is imitating one arm of the human skeleton. My work on this project included understanding how these non-humanoid characters with very low-DOF can perceptually imitate this higher-DOF
                                        skeleton, which can have ramifications on understanding the mode of imitation between two highly different moving bodies.</p>
                                    <p> This work was done under <a href="https://mechanical.illinois.edu/directory/faculty/alaviers">Dr. Amy LaViers</a> at the University of Illinois at Urbana-Champaign. All work was funded by DARPA except the MOCO 2020
                                        work sponspered by Siemens.</p>

                                    <b>Related Publications </b>
                                    <p>[Conference] <b>R. Kaushik</b>, A. K. Mishra, and A. LaViers, “<a href="https://dl.acm.org/doi/abs/10.1145/3401956.3404188">Feasible Stylized Motion: Robotic Manipulator Imitation of a Human Demonstration with Collision Avoidance and Style Parameters in Increasingly Cluttered Environments</a>,” MOCO 2020.</p>
                                    <p>[Journal] <b>R. Kaushik</b> and A. LaViers, <a href="https://link.springer.com/article/10.1007/s12369-019-00595-y">Imitation of Human Motion by Low Degree-of-Freedom Simulated Robots and Human Preference for Mappings Driven by Spinal, Arm, and Leg Activity,</a>, IJSR 2019</p>
                                    <p>[Thesis] <b>R. Kaushik</b>. <a href="https://www.ideals.illinois.edu/handle/2142/104951"> Developing and evaluating a model for human motion to facilitate low degree-of-freedom robot imitation of human movement.</a>                   Masters Thesis. 2019.</p>
                                    <p>[Conference] <b>R. Kaushik</b> and A. LaViers, <a href="http://aisb2019.machinemovementlab.net/MTSB2019_Kaushik_Laviers.pdf">Using verticality to classify motion: analysis of two Indian classical dance styles</a>, presented
                                        at the AISB Symposium on “Movement that Shapes Behaviour”, 2019. </p>
                                    <p>[Conference] <b>R. Kaushik</b> and A. LaViers, “<a href="http://link.springer.com/10.1007/978-3-030-05204-1_58">Imitating Human Movement Using a Measure of Verticality to Animate Low Degree-of-Freedom Non-humanoid Virtual Characters</a>,” ICSR 2018. </p>
                                    <p>[Conference] <b>R. Kaushik</b>, I. Vidrin, and A. LaViers, “<a href="http://dl.acm.org/citation.cfm?doid=3212721.3212805">Quantifying Coordination in Human Dyads via a Measure of Verticality</a>,” MOCO 2018. </p>
                                </div>
                            </div>
                        </div>
                        <div class="card">
                            <div class="card-header" id="surework">
                                <h5 class="mb-0">
                                    <button class="btn btn-link collapsed" type="button" data-toggle="collapse" data-target="#collapseThree" aria-expanded="false" aria-controls="collapseThree">
                                        <div style="white-space:normal">
                                        <span class="float-left">Trajectory Learning from Demonstration: 2016 &nbsp &nbsp</span>
                                        <span class="float-right"><i class="fa"></i></span>
                                        </div>
                                    </button>
                                </h5>
                            </div>
                            <div id="collapseThree" class="collapse" aria-labelledby="surework" data-parent="#accordionExample">
                                <div class="card-body">
                                    <p> This work presents a novel geometric framework for intuitively encoding and learning a wide range of trajectory-based skills from human demonstrations. Our approach identifies and extracts the main characteristics of
                                        the demonstrated skill, which are spatial correlations across different demonstrations. Using the extracted characteristics, the proposed approach generates a continuous representation of the skill based on the
                                        concept of canal surfaces. </p>

                                    <a href="#" style="width: 100%; max-width: 300px; height: auto; display: block; margin: 0 auto; " class="image featured"><img src="images/canalsurface.jpg" alt="" /></a>
                                    <br>
                                    <p> This work was done under <a href="http://www.cc.gatech.edu/~chernova/">Dr. Sonia Chernova </a> at Georgia Tech, with funding provided by the SURE Robotics (Research Experience for Undergraduates) Program. </p>

                                    <b>Related Publications </b>
                                    <p>[Conference] S. R. Ahmadzadeh, <b>R. Kaushik </b>, and S. Chernova, “<a href="http://ieeexplore.ieee.org/document/7803328/">Trajectory learning from demonstration with canal surfaces: A parameter-free approach</a>,” Humanoids 2016. </p>
                                </div>
                            </div>
                        </div>
                    </div>

                </article>
            </div>
        </div>

        <div id="copyright" class="container">
            <ul class="menu">
                <li><a href="https://html5up.net/telephasic">Telephasic Template by HTML5 UP</a></li>
            </ul>
        </div>
    </div>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery.dropotron.min.js"></script>
    <script src="assets/js/browser.min.js"></script>
    <script src="assets/js/breakpoints.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
</body>

</html>